# -*- coding: utf-8 -*-
"""Part3_Итоговый пайплайн.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1e2a-HSdUo8uRy8FDc8dEKH18LCVASHl1

# Итоговый пайплайн для распознавания лиц
"""

import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
import cv2
import os

import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.models as models
import torchvision.transforms as transforms

from google.colab import drive
drive.mount('/content/drive')

"""Загрузим изображения, на которых хотим протестирвать работу пайплайна:"""

image_1 = '/content/drive/MyDrive/ Курсы DLS/Итоговый проект dls/Тестовые изображения/z.png'
image_2 = '/content/drive/MyDrive/ Курсы DLS/Итоговый проект dls/Тестовые изображения/zz.png'
image_3 = '/content/drive/MyDrive/ Курсы DLS/Итоговый проект dls/Тестовые изображения/d.png'

def display_images(img_paths):
    plt.figure(figsize=(15, 5))
    for i, img_path in enumerate(img_paths):
        try:
            img = Image.open(img_path)
            plt.subplot(1, len(img_paths), i + 1)
            plt.imshow(img)
            plt.title(f'Image {i+1}')
            plt.axis('off')
        except FileNotFoundError:
            print(f"Error: Image not found at {img_path}")
        except Exception as e:
            print(f"An error occurred while loading image {img_path}: {e}")
    plt.show()

image_paths = [image_1, image_2, image_3]
display_images(image_paths)

""" Для детекции лиц используем готовую модель из OpenCV: DNN Module"""

# Фреймворк Caffe позволяет работать с моделями глубокого обучения
prototxt_path = 'deploy.prototxt'
caffemodel_path = 'res10_300x300_ssd_iter_140000.caffemodel'

print(f"Checking for {prototxt_path}...")
if not os.path.exists(prototxt_path) or os.path.getsize(prototxt_path) == 0:
    print(f"Downloading {prototxt_path}...")
    !wget https://raw.githubusercontent.com/opencv/opencv/master/samples/dnn/face_detector/deploy.prototxt
print(f"Checking for {caffemodel_path}...")
if not os.path.exists(caffemodel_path) or os.path.getsize(caffemodel_path) == 0:
    print(f"Downloading {caffemodel_path}...")

    !wget https://github.com/opencv/opencv_3rdparty/raw/dnn_samples_face_detector_20170830/res10_300x300_ssd_iter_140000.caffemodel

# Модель DNN Module
face_detector_dnn = None
if os.path.exists(prototxt_path) and os.path.getsize(prototxt_path) > 0 and \
   os.path.exists(caffemodel_path) and os.path.getsize(caffemodel_path) > 0:
    # Загрузим модель
    try:
        face_detector_dnn = cv2.dnn.readNetFromCaffe(prototxt_path, caffemodel_path)
        print("Caffe model loaded successfully.")
    except Exception as e:
        print(f"Error loading Caffe model even with files present: {e}")
else:
    print(f"Error: Missing or empty model files after download attempts. Cannot proceed with face detection. Checked for: {prototxt_path} (size: {os.path.getsize(prototxt_path) if os.path.exists(prototxt_path) else 'N/A'}), {caffemodel_path} (size: {os.path.getsize(caffemodel_path) if os.path.exists(caffemodel_path) else 'N/A'}) ")

def detect_and_crop_faces_dnn(image_path, detector, confidence_threshold=0.5):
    if detector is None:
        print("Face detector not initialized due to missing model files.")
        return None, [], 0
    try:

        pil_img = Image.open(image_path).convert('RGB')
        image = np.array(pil_img)
        (h, w) = image.shape[:2]

        blob = cv2.dnn.blobFromImage(cv2.resize(image, (300, 300)), 1.0,
                                     (300, 300), (104.0, 177.0, 123.0))

        detector.setInput(blob)
        detections = detector.forward()

        annotated_image = image.copy()
        cropped_faces = []
        num_faces = 0

        for i in range(0, detections.shape[2]):
            confidence = detections[0, 0, i, 2]

            if confidence > confidence_threshold:
                num_faces += 1
                box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])
                (startX, startY, endX, endY) = box.astype("int")
                startX = max(0, startX)
                startY = max(0, startY)
                endX = min(w, endX)
                endY = min(h, endY)

                #  Обрезка по bbox
                if (endX - startX > 0) and (endY - startY > 0):
                    cropped_face = image[startY:endY, startX:endX]
                    cropped_faces.append(cropped_face)
                    cv2.rectangle(annotated_image, (startX, startY), (endX, endY),
                                  (0, 255, 0), 2)

        return annotated_image, cropped_faces, num_faces

    except FileNotFoundError:
        print(f"Error: Image not found at {image_path}")
        return None, [], 0
    except Exception as e:
        print(f"An error occurred while processing {image_path}: {e}")
        return None, [], 0

image_paths = [image_1, image_2, image_3]
all_cropped_faces_dnn = []

plt.figure(figsize=(15, 5))
for i, path in enumerate(image_paths):
    processed_img, faces, num_faces = detect_and_crop_faces_dnn(path, face_detector_dnn)
    if processed_img is not None:
        plt.subplot(1, len(image_paths), i + 1)
        plt.imshow(processed_img)
        plt.title(f'Image {i+1} ({num_faces} faces)')
        plt.axis('off')
        all_cropped_faces_dnn.extend(faces)
plt.suptitle('OpenCV DNN Face Detection: Original Images with Bounding Boxes', fontsize=16)
plt.show()

#  Вывод изображений
if all_cropped_faces_dnn:
    plt.figure(figsize=(len(all_cropped_faces_dnn) * 3, 4))
    for i, face in enumerate(all_cropped_faces_dnn):
        plt.subplot(1, len(all_cropped_faces_dnn), i + 1)
        plt.imshow(face)
        plt.title(f'Cropped Face {i+1}')
        plt.axis('off')
    plt.suptitle('OpenCV DNN Face Detection: Cropped Faces', fontsize=16)
    plt.show()
else:
    print("No faces were detected to crop.")

"""Загрузиим веса обученных ранее моделей:"""

Stacked_hourglass_model ='/content/drive/MyDrive/ Курсы DLS/Итоговый проект dls/ models/stacked_hourglass_net.pth'
Recognition_model = '/content/drive/MyDrive/ Курсы DLS/Итоговый проект dls/ models/best_model  CE loss (final_val_loss 0.73).pth' # Модель на CE loss

#  Архитектура моделей Stacked_hourglass_model и Recognition_model
class ResidualBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.skip = nn.Identity() if in_channels == out_channels else nn.Conv2d(in_channels, out_channels, 1)

        self.conv1 = nn.Conv2d(in_channels, out_channels // 2, 1)
        self.bn1 = nn.BatchNorm2d(out_channels // 2)
        self.conv2 = nn.Conv2d(out_channels // 2, out_channels // 2, 3, padding=1)
        self.bn2 = nn.BatchNorm2d(out_channels // 2)
        self.conv3 = nn.Conv2d(out_channels // 2, out_channels, 1)
        self.bn3 = nn.BatchNorm2d(out_channels)

        self.relu = nn.ReLU(inplace=True)

    def forward(self, x):
        residual = self.skip(x)
        x = self.relu(self.bn1(self.conv1(x)))
        x = self.relu(self.bn2(self.conv2(x)))
        x = self.bn3(self.conv3(x))
        return self.relu(x + residual)


class HourglassModule(nn.Module):
    def __init__(self, n, f, bn=None, increase=0):
        super().__init__()
        self.n = n
        nf = f + increase

        self.up1 = ResidualBlock(f, f)
        self.pool1 = nn.MaxPool2d(2, 2)
        self.low1 = ResidualBlock(f, nf)

        if self.n > 1:
            self.low2 = HourglassModule(self.n - 1, nf, bn=bn)
        else:
            self.low2 = ResidualBlock(nf, nf)

        self.low3 = ResidualBlock(nf, f)
        self.up2 = nn.Upsample(scale_factor=2, mode='nearest')

    def forward(self, x):
        up1 = self.up1(x)
        pool1 = self.pool1(x)
        low1 = self.low1(pool1)
        low2 = self.low2(low1)
        low3 = self.low3(low2)
        up2 = self.up2(low3)
        return up1 + up2


class StackedHourglassNet(nn.Module):
    def __init__(self, num_stacks, num_channels, num_landmarks):
        super().__init__()
        self.num_stacks = num_stacks
        self.num_channels = num_channels
        self.num_landmarks = num_landmarks

        self.initial_features = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            ResidualBlock(64, 128),
            nn.MaxPool2d(2, 2),
            ResidualBlock(128, num_channels),
            ResidualBlock(num_channels, num_channels)
        )

        # Stacked Hourglass Blocks
        self._hourglass = nn.ModuleList()
        self._intermediate_res = nn.ModuleList()
        self._heatmap_heads = nn.ModuleList()
        self._feature_convs = nn.ModuleList()
        self._heatmap_convs = nn.ModuleList()

        for i in range(num_stacks):
            self._hourglass.append(HourglassModule(n=4, f=num_channels))
            self._intermediate_res.append(ResidualBlock(num_channels, num_channels))
            self._heatmap_heads.append(nn.Conv2d(num_channels, num_landmarks, kernel_size=1))

            if i < num_stacks - 1:
                self._feature_convs.append(nn.Conv2d(num_channels, num_channels, kernel_size=1))
                self._heatmap_convs.append(nn.Conv2d(num_landmarks, num_channels, kernel_size=1))

    def forward(self, x):
        x = self.initial_features(x)

        outputs = []
        for i in range(self.num_stacks):
            hg_output = self._hourglass[i](x)
            hg_output = self._intermediate_res[i](hg_output)

            heatmap_output = self._heatmap_heads[i](hg_output)
            outputs.append(heatmap_output)

            if i < self.num_stacks - 1:
                feature_proj = self._feature_convs[i](hg_output)
                heatmap_proj = self._heatmap_convs[i](heatmap_output)
                x = x + feature_proj + heatmap_proj

        return outputs


def accuracy(outputs, labels):
    _, preds = torch.max(outputs, dim=1)
    return torch.tensor(torch.sum(preds == labels).item() / len(preds))

class ImageClassificationBase(nn.Module):
    def training_step(self, batch):
        images, labels = batch
        images = images.to(device)
        labels = labels.to(device)
        out = self(images)
        loss = F.cross_entropy(out, labels)
        acc = accuracy(out, labels)
        return loss,acc

    def validation_step(self, batch):
        images, labels = batch
        images = images.to(device)
        labels = labels.to(device)
        out = self(images)
        loss = F.cross_entropy(out, labels)
        acc = accuracy(out, labels)
        return {'val_loss': loss.detach(), 'val_acc': acc}

    def validation_epoch_end(self, outputs):
        batch_losses = [x['val_loss'] for x in outputs]
        epoch_loss = torch.stack(batch_losses).mean()
        batch_accs = [x['val_acc'] for x in outputs]
        epoch_acc = torch.stack(batch_accs).mean()
        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}

    def epoch_end(self, epoch, result):
        print("Epoch [{}], train_loss: {:.4f}, train_acc: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}, last_lr: {:.5f}".format(
            epoch+1, result['train_loss'], result['train_accuracy'], result['val_loss'], result['val_acc'], result['lrs'][-1]))


class ResNet18Transfer(ImageClassificationBase):
    def __init__(self, num_classes):
        super().__init__()

        self.resnet = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)

        in_features = self.resnet.fc.in_features
        self.resnet.fc = nn.Linear(in_features, num_classes)

    def forward(self, xb):
        return self.resnet(xb)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f'Using device: {device}')

# Параметры дял Stacked Hourglass
num_stacks_hg = 2
num_channels_hg_corrected = 128
num_landmarks_hg_corrected = 5

hourglass_model = StackedHourglassNet(num_stacks_hg, num_channels_hg_corrected, num_landmarks_hg_corrected).to(device)
print(f'StackedHourglassNet instantiated with num_stacks={num_stacks_hg}, num_channels={num_channels_hg_corrected}, num_landmarks={num_landmarks_hg_corrected}')

#  Модель ResNet18Transfer
num_classes_resnet = 1500 # Corrected num_classes as per subtask
resnet_model = ResNet18Transfer(num_classes_resnet).to(device)
print(f'ResNet18Transfer instantiated with num_classes={num_classes_resnet}')

#  Веса StackedHourglassNet
try:
    hourglass_model_state_dict = torch.load(Stacked_hourglass_model, map_location=torch.device('cpu'))
    hourglass_model.load_state_dict(hourglass_model_state_dict, strict=False)
    print(f'StackedHourglassNet weights loaded from {Stacked_hourglass_model} with strict=False.')
except Exception as e:
    print(f"Error loading StackedHourglassNet weights from {Stacked_hourglass_model}: {e}")

# 5. Веса ResNet18Transfer
try:
    resnet_model_state_dict = torch.load(Recognition_model, map_location=torch.device('cpu'))
    resnet_model.load_state_dict(resnet_model_state_dict)
    print(f'ResNet18Transfer weights loaded from {Recognition_model}')
except Exception as e:
    print(f"Error loading ResNet18Transfer weights from {Recognition_model}: {e}")

# Режим eval
hourglass_model.eval()
resnet_model.eval()
print('Both models set to evaluation mode.')

import torchvision.transforms as transforms

# Определить предварительную обработку для StackedHourglassNet

preprocess_hourglass = transforms.Compose([
    transforms.Resize((256, 256)),
    transforms.Grayscale(num_output_channels=3),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
])

preprocess_resnet = transforms.Compose([
    transforms.ToPILImage(),
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

print("Image preprocessing transforms defined.")

""" Функция для получения эмбендингов:"""

def get_face_embedding(image_path, face_detector, resnet_model, preprocess_resnet, device):

    # Обнаружение и обрезка лиц с помощью детектора лиц DNN
    processed_img, cropped_faces, num_faces = detect_and_crop_faces_dnn(image_path, face_detector)

    if num_faces == 0:
        print(f"No faces detected in {image_path}")
        return None

    face = cropped_faces[0]
    face_pil = Image.fromarray(face)

    #  Предобработка
    input_tensor = preprocess_resnet(face_pil)
    input_batch = input_tensor.unsqueeze(0).to(device)

    #  Получаем эмбенденги
    with torch.no_grad():

        original_fc = resnet_model.resnet.fc
        resnet_model.resnet.fc = nn.Identity()
        embedding = resnet_model(input_batch).squeeze(0)

        resnet_model.resnet.fc = original_fc

    return embedding.cpu()

print("Function 'get_face_embedding' defined.")

"""Получим эмбенденги для каждого изображения:"""

from sklearn.metrics.pairwise import cosine_similarity

print(f"Generating embedding for {image_1}")
embedding_z = get_face_embedding(image_1, face_detector_dnn, resnet_model, preprocess_resnet, device)

print(f"Generating embedding for {image_2}")
embedding_zz = get_face_embedding(image_2, face_detector_dnn, resnet_model, preprocess_resnet, device)

print(f"Generating embedding for {image_3}")
embedding_r = get_face_embedding(image_3, face_detector_dnn, resnet_model, preprocess_resnet, device)

#  Расчёт косинусного сходства
if embedding_z is not None and embedding_zz is not None:
    embedding_z_np = embedding_z.numpy().reshape(1, -1)
    embedding_zz_np = embedding_zz.numpy().reshape(1, -1)
    cosine_dist_z_zz = 1 - cosine_similarity(embedding_z_np, embedding_zz_np)[0][0]
    print(f"Cosine distance between 'z.png' and 'zz.png': {cosine_dist_z_zz:.4f}")
else:
    print("Could not calculate cosine distance between 'z.png' and 'zz.png' due to missing embeddings.")

if embedding_z is not None and embedding_r is not None:
    embedding_r_np = embedding_r.numpy().reshape(1, -1)
    cosine_dist_z_r = 1 - cosine_similarity(embedding_z_np, embedding_r_np)[0][0]
    print(f"Cosine distance between 'z.png' and 'r.png': {cosine_dist_z_r:.4f}")
else:
    print("Could not calculate cosine distance between 'z.png' and 'r.png' due to missing embeddings.")

print("Summary: Models loaded, embeddings generated, and cosine distances calculated.")

"""Выведим полученный результ:"""

plt.figure(figsize=(10, 5))

plt.subplot(1, 2, 1)
img1 = Image.open(image_1)
plt.imshow(img1)
plt.title('Image 1 (z.png)')
plt.axis('off')

plt.subplot(1, 2, 2)
img2 = Image.open(image_2)
plt.imshow(img2)
plt.title('Image 2 (zz.png)')
plt.axis('off')

plt.suptitle(f"Cosine distance between Image 1 and Image 2: {cosine_dist_z_zz:.4f}", fontsize=12)
plt.show()

plt.figure(figsize=(10, 5))

plt.subplot(1, 2, 1)
img1 = Image.open(image_1)
plt.imshow(img1)
plt.title('Image 1 (z.png)')
plt.axis('off')

plt.subplot(1, 2, 2)
img3 = Image.open(image_3)
plt.imshow(img3)
plt.title('Image 3 (t.png)')
plt.axis('off')

plt.suptitle(f"Cosine distance between Image 1 and Image 3: {cosine_dist_z_r:.4f}", fontsize=12)
plt.show()
