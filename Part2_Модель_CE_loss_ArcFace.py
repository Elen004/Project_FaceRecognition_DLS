# -*- coding: utf-8 -*-
"""ArcFace_DLS_Проект(3)_Обучение_модели (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QUbyC9ZVfmYEQTpcO6ILDsx7-3kHNlIu

# Обучение модели классификации лиц на CE Loss и ArcFace Loss
"""

import pandas as pd
import numpy as np
import torch
import torchvision.transforms as tt
import matplotlib.pyplot as plt
from PIL import Image
import os
from torch.utils.data import Dataset
from tqdm.notebook import tqdm
from sklearn.model_selection import train_test_split
import torchvision.models as models
import torch.nn as nn
import torch.nn.functional as F

from google.colab import drive
drive.mount('/content/drive')

"""# Дополнительная подготовка данных для обучения модели

Загрузим и визуализируем данные с которыми предстоит работать:

1) Загрузим файл, содержащий информацию о классах изображений (identity изображений)
"""

full_identity = pd.read_csv('/content/drive/MyDrive/ Курсы DLS/Итоговый проект dls/ Датасет_вариант 2/selected_image_ids_2.txt') # id выравненных и обрезаных изображений, полученных в результате выполнения первой части задания

full_identity

"""Корректное отображение файла:"""

full_identity = pd.read_csv('/content/drive/MyDrive/ Курсы DLS/Итоговый проект dls/ Датасет для второй части задания  (DLS_Проект_(3))/identity_CelebA.txt', header=None)


full_identity.columns = ['combined_info']
split_data = full_identity['combined_info'].str.split(' ', n=1, expand=True)

full_identity['filename'] = split_data[0]
full_identity['id'] = split_data[1]

full_identity['id'] = pd.to_numeric(full_identity['id'])
full_identity = full_identity.drop(columns=['combined_info'])

print(full_identity.head())

"""Загрузим выравненные и обрезанные изображения:"""

cropped_aligned_dataset_path = '/content/drive/MyDrive/ Курсы DLS/Итоговый проект dls/ Датасет_вариант 2/aligned_faces_full_dataset.zip'

import zipfile
import os

def file_from_zip(zip_path):

  extract_path = '/content/selected_images'

  os.makedirs(extract_path, exist_ok=True)

  #  Разархивация
  with zipfile.ZipFile(zip_path, 'r') as zip_ref:
      zip_ref.extractall(extract_path)
  print(f"Файл успешно разархивирован в: {extract_path}")

  cropped_aligned_dataset = cropped_aligned_dataset_path

  return cropped_aligned_dataset

"""Посмотрим на изображения, с которыми предстоит работать:"""

file_from_zip(cropped_aligned_dataset_path)
cropped_aligned_dataset = '/content/selected_images'

image_size = 128
stats = ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])

#  Список изображений
image_files = [os.path.join(cropped_aligned_dataset, f) for f in os.listdir(cropped_aligned_dataset) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]

# Сколько изображений отобразим
num_images_to_show = 4
selected_images = image_files[:num_images_to_show]

# Преобразование изображений в tensor
load_and_transform = tt.Compose([
    tt.Resize(image_size),
    tt.CenterCrop(image_size),
    tt.ToTensor(),
])

# Денормализация
def denormalize(image_tensor, means, stds):
    means = torch.tensor(means).reshape(3, 1, 1)
    stds = torch.tensor(stds).reshape(3, 1, 1)
    return image_tensor * stds + means

plt.figure(figsize=(12, 3))

for i, img_path in enumerate(selected_images):
    #  Подгружаем изображение
    img = Image.open(img_path).convert('RGB')

    #  Выполняем преобразование
    img_tensor = load_and_transform(img)

    #  Нормализуем
    normalized_img_tensor = tt.Normalize(*stats)(img_tensor)

    #  Денормализуем для отображения
    denormalized_img = denormalize(normalized_img_tensor, stats[0], stats[1])


    img_np = np.clip(denormalized_img.permute(1, 2, 0).numpy(), 0, 1)

    plt.subplot(1, num_images_to_show, i + 1)
    plt.imshow(img_np)
    plt.title(os.path.basename(img_path))
    plt.axis('off')

plt.tight_layout()
plt.show()

"""Составим таблицу соответствия для выбранных файлов и их id:"""

image_files_in_dataset = [f for f in os.listdir(cropped_aligned_dataset) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]
base_filenames_in_dataset = {f.replace('aligned_', '') for f in image_files_in_dataset}

filtered_full_identity = full_identity[full_identity['filename'].isin(base_filenames_in_dataset)]
filtered_ids_for_dataset = filtered_full_identity['id']

print(f"Количество найденных ID: {len(filtered_ids_for_dataset)}")
print(filtered_ids_for_dataset.head())

aligned_filenames_df = pd.DataFrame({
    'aligned_filename': image_files_in_dataset,
    'filename': [f.replace('aligned_', '') for f in image_files_in_dataset]
})

filtered_full_identity = pd.merge(filtered_full_identity, aligned_filenames_df, on='filename', how='left')

print("Обновленная таблица filtered_full_identity с наименованиями изображений:")
print(filtered_full_identity.head())

filtered_full_identity = filtered_full_identity.drop(columns=['aligned_filename'])
print("Обновленная таблица filtered_full_identity после удаления aligned_filename:")
print(filtered_full_identity.head())
# filtered_full_identity - итоговая таблица

"""Посмотрим на распределение классов в итоговом датасете:"""

import matplotlib.pyplot as plt

id_filename_counts = filtered_full_identity.groupby('id')['filename'].nunique()

plt.figure(figsize=(12, 6))
id_filename_counts.plot(kind='hist', bins=range(1, id_filename_counts.max() + 2), edgecolor='black')
plt.title('Распределение количества уникальных изображений на ID')
plt.xlabel('Количество уникальных изображений')
plt.ylabel('Количество ID')
plt.xticks(range(1, id_filename_counts.max() + 1))
plt.grid(axis='y', alpha=0.75)
plt.show()

print("Статистика по количеству уникальных изображений на ID:")
print(id_filename_counts.describe())

"""**Характеристика итогового датасета для обучения модели:**

датасет для оучения состоит из 20 000 изображений, которые поделены на 1500 классов. Для каждого класса отобрано 10, 11 или 15 изображений. В качестве "классов" были использованы id людей, чьи лица представлены на изображениях.
"""

class FaceDataset(Dataset):
    def __init__(self, image_dir, dataframe, transform=None):
        """
            image_dir (str): Каталог со всеми изображениями.
            dataframe (pd.DataFrame): таблица, содержащая наименования изображений и соответсвующие id.

        """
        self.image_dir = image_dir
        self.dataframe = dataframe
        self.transform = transform

    def __len__(self):
        return len(self.dataframe)

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()

        img_name = os.path.join(self.image_dir, self.dataframe.iloc[idx]['filename'])
        actual_img_path = os.path.join(self.image_dir, 'aligned_' + self.dataframe.iloc[idx]['filename'])


        if os.path.exists(actual_img_path):
            img_path = actual_img_path
        else:
            img_path = os.path.join(self.image_dir, self.dataframe.iloc[idx]['filename'])

            if not os.path.exists(img_path):
                raise FileNotFoundError(f"Image file not found at {actual_img_path} or {img_path}")

        image = Image.open(img_path).convert('RGB')
        label = self.dataframe.iloc[idx]['encoded_id']

        if self.transform:
            image = self.transform(image)

        return image, label

print("FaceDataset class defined successfully.")

"""Сопоставим id из filtered_full_identity с числовыми метками (от 0 до N-1):"""

unique_ids = filtered_full_identity['id'].unique()
id_to_label = {original_id: new_label for new_label, original_id in enumerate(unique_ids)}

filtered_full_identity = filtered_full_identity.copy()
filtered_full_identity['encoded_id'] = filtered_full_identity['id'].map(id_to_label)

num_classes = len(unique_ids)

print("Обновленная таблица filtered_full_identity с колонкой 'encoded_id':")
print(filtered_full_identity.head())
print(f"\nОбщее количество уникальных классов (num_classes): {num_classes}")

"""Преобразование и разделение данных:"""

import torchvision.transforms as tt
from sklearn.model_selection import train_test_split

# Преобразования
train_transform = tt.Compose([
    tt.RandomHorizontalFlip(p=0.5),
    tt.RandomRotation(degrees=15),
    tt.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    tt.Resize(image_size),
    tt.ToTensor(),
    tt.Normalize(*stats)
])

val_transform = tt.Compose([
    tt.Resize(image_size),
    tt.ToTensor(),
    tt.Normalize(*stats)
])

# Разделим ддатасет на тестовую и валидационную выборки

class_counts = filtered_full_identity['encoded_id'].value_counts()
min_samples_for_stratified_split = 5

classes_for_training_only = class_counts[class_counts < min_samples_for_stratified_split].index

df_training_only = filtered_full_identity[filtered_full_identity['encoded_id'].isin(classes_for_training_only)]
df_splittable = filtered_full_identity[~filtered_full_identity['encoded_id'].isin(classes_for_training_only)]

train_splittable_df, val_splittable_df = train_test_split(
    df_splittable,
    test_size=0.2,
    random_state=42,
    stratify=df_splittable['encoded_id']
)

train_df = pd.concat([train_splittable_df, df_training_only])
val_df = val_splittable_df

print(f"Original total records: {len(filtered_full_identity)}")
print(f"Classes with less than {min_samples_for_stratified_split} instances (training only): {len(classes_for_training_only)}")
print(f"Records in training_only dataframe: {len(df_training_only)}")
print(f"Records in splittable dataframe: {len(df_splittable)}")
print(f"Training set size: {len(train_df)} records (includes all small classes)")
print(f"Validation set size: {len(val_df)} records (only from larger classes)")

train_dataset = FaceDataset(image_dir=cropped_aligned_dataset, dataframe=train_df, transform=train_transform)
val_dataset = FaceDataset(image_dir=cropped_aligned_dataset, dataframe=val_df, transform=val_transform)

print("Train and Validation datasets created successfully.")

"""Создадим DataLoaders для трейна и валидации:

"""

from torch.utils.data import DataLoader

batch_size = 64

num_workers = 2
pin_memory = torch.cuda.is_available()

train_dl = DataLoader(train_dataset, batch_size, shuffle=True, num_workers=num_workers, pin_memory=pin_memory)

val_dl = DataLoader(val_dataset, batch_size, shuffle=False, num_workers=num_workers, pin_memory=pin_memory)

print(f"Training DataLoader created with batch_size={batch_size}, shuffle=True, num_workers={num_workers}, pin_memory={pin_memory}.")
print(f"Validation DataLoader created with batch_size={batch_size}, shuffle=False, num_workers={num_workers}, pin_memory={pin_memory}.")

"""## Определим модель

За основу возьмём ахитектуру модели ResNet 18 (https://github.com/Moddy2024/ResNet-18)
"""

import torch.nn as nn
import torch.nn.functional as F

class ImageClassificationBase(nn.Module):
    def training_step(self, batch):
        images, labels = batch
        images = images.to(device)
        labels = labels.to(device)
        out = self(images)
        loss = F.cross_entropy(out, labels)
        acc = accuracy(out, labels)
        return loss,acc

    def validation_step(self, batch):
        images, labels = batch
        images = images.to(device)
        labels = labels.to(device)
        out = self(images)
        loss = F.cross_entropy(out, labels)
        acc = accuracy(out, labels)
        return {'val_loss': loss.detach(), 'val_acc': acc}

    def validation_epoch_end(self, outputs):
        batch_losses = [x['val_loss'] for x in outputs]
        epoch_loss = torch.stack(batch_losses).mean()
        batch_accs = [x['val_acc'] for x in outputs]
        epoch_acc = torch.stack(batch_accs).mean()
        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}

    def epoch_end(self, epoch, result):
        print("Epoch [{}], train_loss: {:.4f}, train_acc: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}, last_lr: {:.5f}".format(
            epoch+1, result['train_loss'], result['train_accuracy'], result['val_loss'], result['val_acc'], result['lrs'][-1]))

def conv_block(in_channels, out_channels, activation=False, pool=False):

    layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),
              nn.BatchNorm2d(out_channels)]
    if activation: layers.append(nn.ReLU(inplace=True))
    if pool: layers.append(nn.MaxPool2d(2))
    return nn.Sequential(*layers)

class ResNet18(ImageClassificationBase):
    def __init__(self, in_channels, num_classes):
        super().__init__()

        self.conv1 = nn.Sequential(nn.Conv2d(in_channels, 64, kernel_size=7, stride=1, padding=4, bias=False),
            nn.BatchNorm2d(64),nn.ReLU(inplace=True))

        self.res1 = nn.Sequential(conv_block(64, 64,activation=True), conv_block(64, 64))
        self.res2 = nn.Sequential(conv_block(64, 64,activation=True), conv_block(64, 64))

        self.downsample1=nn.Sequential(conv_block(64, 128,pool=True))
        self.res3 = nn.Sequential(conv_block(64, 128,activation=True, pool=True),
                                  conv_block(128,128))
        self.res4 = nn.Sequential(conv_block(128, 128,activation=True), conv_block(128, 128,activation=True))

        self.res5 = nn.Sequential(conv_block(128, 256,activation=True, pool=True),conv_block(256,256))
        self.downsample2 = nn.Sequential(conv_block(128, 256,pool=True,activation=True))
        self.res6 = nn.Sequential(conv_block(256, 256,activation=True), conv_block(256, 256,activation=True))

        self.res7 = nn.Sequential(conv_block(256, 512,activation=True, pool=True),
                                   conv_block(512,512,activation=True))
        self.downsample3 = nn.Sequential(conv_block(256,512,activation=True,pool=True))
        self.res8 = nn.Sequential(conv_block(512, 512,activation=True), conv_block(512, 512,activation=True))

        self.classifier = nn.Sequential(nn.AdaptiveAvgPool2d((1,1)),
                                        nn.Flatten(),
                                        nn.Dropout(0.1),
                                        nn.Linear(512, num_classes))

        self.apply(self.init_weights)

    def init_weights(self,m):
        if isinstance(m, nn.Conv2d):
            nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')


    def forward(self, xb):
        out = self.conv1(xb)
        out = self.res1(out) + out
        out = self.res2(out) + out
        out = self.res3(out) + self.downsample1(out)
        out = self.res4(out) + out
        out = self.res5(out) + self.downsample2(out)
        out = self.res6(out) + out
        out = self.downsample3(out) +self.res7(out)
        out = self.res8(out) + out
        out = self.classifier(out)
        return out

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

in_channels = 3
model = ResNet18(in_channels=in_channels, num_classes=num_classes)

model.to(device);
print(f"ResNet18 model instantiated with {num_classes} output classes and moved to {device}.")

max_lr = 0.001
optimizer = torch.optim.AdamW(model.parameters(), lr=max_lr, weight_decay=0.01)

print(f"Optimizer (AdamW) defined with learning rate: {max_lr}")

"""Обучим данную модель (CE loss):"""

def accuracy(outputs, labels):
    _, preds = torch.max(outputs, dim=1)
    return torch.tensor(torch.sum(preds == labels).item() / len(preds))

def get_lr(optimizer):
    for param_group in optimizer.param_groups:
        return param_group['lr']

@torch.no_grad()
def evaluate(model, val_loader):
    model.eval()
    outputs = []
    for batch in val_loader:
        outputs.append(model.validation_step(batch))
    return model.validation_epoch_end(outputs)

def fit_one_cycle(epochs, max_lr, model, train_loader, val_loader, optimizer, grad_clip=None, weight_decay=0, start_pct=0.3, moms=(0.95, 0.85), pct_start=0.3, div_factor=25, final_div_factor=1e4):
    torch.cuda.empty_cache()
    history = []

    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr,
                                                    epochs=epochs, steps_per_epoch=len(train_loader),
                                                    pct_start=pct_start,
                                                    div_factor=div_factor,
                                                    final_div_factor=final_div_factor,
                                                    anneal_strategy='cos')

    best_val_acc = -np.inf
    best_model_path = 'best_model.pth'

    for epoch in range(epochs):
        model.train()
        train_losses = []
        train_accs = []
        lrs = []

        for batch in tqdm(train_loader):
            images, labels = batch
            images = images.to(device)
            labels = labels.to(device)

            optimizer.zero_grad()

            out = model(images)
            loss = F.cross_entropy(out, labels)
            loss.backward()

            if grad_clip:
                nn.utils.clip_grad_value_(model.parameters(), grad_clip)

            optimizer.step()

            train_losses.append(loss.item())
            train_accs.append(accuracy(out, labels).item()) # Calculate accuracy

            lrs.append(get_lr(optimizer))
            scheduler.step()

        # Валидация
        result = evaluate(model, val_loader)
        result['train_loss'] = np.mean(train_losses)
        result['train_accuracy'] = np.mean(train_accs)
        result['lrs'] = lrs
        model.epoch_end(epoch, result)
        history.append(result)

        #  Сохранение модели
        if result['val_acc'] > best_val_acc:
            best_val_acc = result['val_acc']
            torch.save(model.state_dict(), best_model_path)
            print(f"Saving best model with validation accuracy: {best_val_acc:.4f}")

    return history

# GPU
if not torch.cuda.is_available():
    raise RuntimeError("CUDA is not available. Please enable GPU runtime in Colab settings to train on GPU.")

#  Параметры обучения
epochs = 20
grad_clip = 0.1
weight_decay = 1e-4
max_lr = 0.001

history = fit_one_cycle(epochs, max_lr, model, train_dl, val_dl, optimizer, grad_clip=grad_clip, weight_decay=weight_decay)

print("Model training complete!")

from google.colab import files
files.download('best_model.pth')

"""**Оценка обучения:**

На данный момент accuarcy на валидационной выборке составил всего 0,3842. Модель нуждается в улучшении. Попробуем использовать предобученную модель ResNet18

# Обучение модели ResNet18 с предобучением (CE loss)
"""

train_transform = tt.Compose([
    tt.RandomHorizontalFlip(p=0.5),
    tt.RandomRotation(degrees=30),
    tt.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.2),
    tt.RandomPerspective(distortion_scale=0.2, p=0.5),
    tt.Resize(image_size),
    tt.ToTensor(),
    tt.Normalize(*stats)
])

val_transform = tt.Compose([
    tt.Resize(image_size),
    tt.ToTensor(),
    tt.Normalize(*stats)
])

class_counts = filtered_full_identity['encoded_id'].value_counts()
min_samples_for_stratified_split = 5

classes_for_training_only = class_counts[class_counts < min_samples_for_stratified_split].index

df_training_only = filtered_full_identity[filtered_full_identity['encoded_id'].isin(classes_for_training_only)]
df_splittable = filtered_full_identity[~filtered_full_identity['encoded_id'].isin(classes_for_training_only)]

train_splittable_df, val_splittable_df = train_test_split(
    df_splittable,
    test_size=0.2,
    random_state=42,
    stratify=df_splittable['encoded_id']
)

train_df = pd.concat([train_splittable_df, df_training_only])
val_df = val_splittable_df

print(f"Original total records: {len(filtered_full_identity)}")
print(f"Classes with less than {min_samples_for_stratified_split} instances (training only): {len(classes_for_training_only)}")
print(f"Records in training_only dataframe: {len(df_training_only)}")
print(f"Records in splittable dataframe: {len(df_splittable)}")
print(f"Training set size: {len(train_df)} records (includes all small classes)")
print(f"Validation set size: {len(val_df)} records (only from larger classes)")

train_dataset = FaceDataset(image_dir=cropped_aligned_dataset, dataframe=train_df, transform=train_transform)
val_dataset = FaceDataset(image_dir=cropped_aligned_dataset, dataframe=val_df, transform=val_transform)

print("Train and Validation datasets created successfully.")

def accuracy(outputs, labels):
    _, preds = torch.max(outputs, dim=1)
    return torch.tensor(torch.sum(preds == labels).item() / len(preds))

class ImageClassificationBase(nn.Module):
    def training_step(self, batch):
        images, labels = batch
        images = images.to(device)
        labels = labels.to(device)
        out = self(images)
        loss = F.cross_entropy(out, labels)
        acc = accuracy(out, labels)
        return loss,acc

    def validation_step(self, batch):
        images, labels = batch
        images = images.to(device)
        labels = labels.to(device)
        out = self(images)
        loss = F.cross_entropy(out, labels)
        acc = accuracy(out, labels)
        return {'val_loss': loss.detach(), 'val_acc': acc}

    def validation_epoch_end(self, outputs):
        batch_losses = [x['val_loss'] for x in outputs]
        epoch_loss = torch.stack(batch_losses).mean()
        batch_accs = [x['val_acc'] for x in outputs]
        epoch_acc = torch.stack(batch_accs).mean()
        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}

    def epoch_end(self, epoch, result):
        print("Epoch [{}], train_loss: {:.4f}, train_acc: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}, last_lr: {:.5f}".format(
            epoch+1, result['train_loss'], result['train_accuracy'], result['val_loss'], result['val_acc'], result['lrs'][-1]))


# Предобученная ResNet18
class ResNet18Transfer(ImageClassificationBase):
    def __init__(self, num_classes):
        super().__init__()

        self.resnet = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)

        in_features = self.resnet.fc.in_features
        self.resnet.fc = nn.Linear(in_features, num_classes)

    def forward(self, xb):
        return self.resnet(xb)



model_transfer = ResNet18Transfer(num_classes=num_classes)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model_transfer.to(device);

print(f"Pre-trained ResNet18 model loaded and adapted for {num_classes} classes.")
print(f"Model moved to device: {device}.")

optimizer_transfer = torch.optim.AdamW(model_transfer.parameters(), lr=max_lr, weight_decay=weight_decay)

print(f"New optimizer (AdamW) initialized for the transfer learning model with learning rate: {max_lr}")

history_transfer = fit_one_cycle(epochs, max_lr, model_transfer, train_dl, val_dl, optimizer_transfer, grad_clip=grad_clip, weight_decay=weight_decay)

print("Transfer learning training complete!")

files.download('best_model.pth')

"""---

**Оценка обучения ResNet18 с предобучением на CE loss:**

Validation accuracy составила 0.7383, что значительно превышает accuarcy модели без предобучения (0.3842)

#ArcFace Loss

Определим функцию ArcFace и выполним обучение модели ResNet18 (с предобучением) на ArcFace loss:
"""

class ArcFace(nn.Module):
    def __init__(self, in_features, num_classes, s=30.0, m=0.5):
        super().__init__()
        self.s = s
        self.m = m
        self.weight = nn.Parameter(torch.randn(num_classes, in_features))
        nn.init.xavier_uniform_(self.weight)
        self.num_classes = num_classes

    def forward(self, embeddings, labels):
        embeddings = F.normalize(embeddings)
        W = F.normalize(self.weight)

        cos_theta = torch.matmul(embeddings, W.t()).clamp(-1, 1)

        if self.training:

            theta = torch.acos(cos_theta)
            cos_theta_m = torch.cos(theta + self.m)

            one_hot = F.one_hot(labels, num_classes=self.num_classes).float()
            output_cos = one_hot * cos_theta_m + (1 - one_hot) * cos_theta

        else:
            output_cos = cos_theta

        logits = self.s * output_cos
        return logits

import torch.nn as nn
import torch.nn.functional as F

def accuracy(outputs, labels):
    _, preds = torch.max(outputs, dim=1)
    return torch.tensor(torch.sum(preds == labels).item() / len(preds))

class ImageClassificationBase(nn.Module):
    def training_step(self, batch):
        images, labels = batch
        images = images.to(device)
        labels = labels.to(device)

        out = self(images, labels)
        loss = F.cross_entropy(out, labels)
        acc = accuracy(out, labels)
        return loss, acc

    def validation_step(self, batch):
        images, labels = batch
        images = images.to(device)
        labels = labels.to(device)
        out = self(images, labels)
        loss = F.cross_entropy(out, labels)
        acc = accuracy(out, labels)
        return {'val_loss': loss.detach(), 'val_acc': acc}

    def validation_epoch_end(self, outputs):
        batch_losses = [x['val_loss'] for x in outputs]
        epoch_loss = torch.stack(batch_losses).mean()
        batch_accs = [x['val_acc'] for x in outputs]
        epoch_acc = torch.stack(batch_accs).mean()
        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}

    def epoch_end(self, epoch, result):
        print("Epoch [{}], train_loss: {:.4f}, train_acc: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}, last_lr: {:.5f}".format(
            epoch+1, result['train_loss'], result['train_accuracy'], result['val_loss'], result['val_acc'], result['lrs'][-1]))

print("ImageClassificationBase class redefined to pass labels to forward method.")

class ResNet18Embedder(nn.Module):
    def __init__(self):
        super().__init__()

        resnet = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)
        self.features = nn.Sequential(*list(resnet.children())[:-1])

    def forward(self, x):

        x = self.features(x)
        x = x.view(x.size(0), -1)
        return x

print("ResNet18Embedder class defined successfully.")

def accuracy(outputs, labels):
    _, preds = torch.max(outputs, dim=1)
    return torch.tensor(torch.sum(preds == labels).item() / len(preds))

def get_lr(optimizer):
    for param_group in optimizer.param_groups:
        return param_group['lr']

@torch.no_grad()
def evaluate(model, val_loader):
    model.eval()
    outputs = []
    for batch in val_loader:
        outputs.append(model.validation_step(batch))
    return model.validation_epoch_end(outputs)

def fit_one_cycle(epochs, max_lr, model, train_loader, val_loader, optimizer, grad_clip=None, weight_decay=0, start_pct=0.3, moms=(0.95, 0.85), pct_start=0.3, div_factor=25, final_div_factor=1e4):
    torch.cuda.empty_cache()
    history = []

    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr,
                                                    epochs=epochs, steps_per_epoch=len(train_loader),
                                                    pct_start=pct_start,
                                                    div_factor=div_factor,
                                                    final_div_factor=final_div_factor,
                                                    anneal_strategy='cos')

    best_val_acc = -np.inf
    best_model_path = 'best_model_arcface.pth'

    for epoch in range(epochs):
        model.train()
        train_losses = []
        train_accs = []
        lrs = []

        for batch in tqdm(train_loader):
            optimizer.zero_grad()
            loss, acc = model.training_step(batch)
            loss.backward()

            if grad_clip:
                nn.utils.clip_grad_value_(model.parameters(), grad_clip)

            optimizer.step()


            train_losses.append(loss.item())
            train_accs.append(acc.item())

            lrs.append(get_lr(optimizer))
            scheduler.step()

        # Валидация
        result = evaluate(model, val_loader)
        result['train_loss'] = np.mean(train_losses)
        result['train_accuracy'] = np.mean(train_accs)
        result['lrs'] = lrs
        model.epoch_end(epoch, result)
        history.append(result)

        if result['val_acc'] > best_val_acc:
            best_val_acc = result['val_acc']
            torch.save(model.state_dict(), best_model_path)
            print(f"Saving best ArcFace model with validation accuracy: {best_val_acc:.4f}")

    return history

if not torch.cuda.is_available():
    raise RuntimeError("CUDA is not available. Please enable GPU runtime in Colab settings to train on GPU.")

epochs = 20
grad_clip = 0.1
weight_decay = 1e-4
max_lr = 0.001

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

arcface_model = ArcFaceCombinedModel(num_classes=num_classes)
arcface_model.to(device);

print(f"ArcFaceCombinedModel instantiated for {num_classes} classes and moved to {device}.")

optimizer_arcface = torch.optim.AdamW(arcface_model.parameters(), lr=max_lr, weight_decay=weight_decay)

print(f"New optimizer (AdamW) initialized for the ArcFace model with learning rate: {max_lr}")

history_arcface = fit_one_cycle(epochs, max_lr, arcface_model, train_dl, val_dl, optimizer_arcface, grad_clip=grad_clip, weight_decay=weight_decay)

print("ArcFace model training complete!")

final_model_path = 'final_arcface_model.pth'
torch.save(arcface_model.state_dict(), final_model_path)
print(f"Final ArcFace model saved to {final_model_path}")

"""Значение accuarcy слишко мало (0.3363). Доработка модели:"""

def accuracy(outputs, labels):
    _, preds = torch.max(outputs, dim=1)
    return torch.tensor(torch.sum(preds == labels).item() / len(preds))

class ImageClassificationBase(nn.Module):
    def training_step(self, batch):
        images, labels = batch
        images = images.to(device)
        labels = labels.to(device)
        out = self(images, labels)
        loss = F.cross_entropy(out, labels)
        acc = accuracy(out, labels)
        return loss, acc

    def validation_step(self, batch):
        images, labels = batch
        images = images.to(device)
        labels = labels.to(device)
        out = self(images, labels)
        loss = F.cross_entropy(out, labels)
        acc = accuracy(out, labels)
        return {'val_loss': loss.detach(), 'val_acc': acc}

    def validation_epoch_end(self, outputs):
        batch_losses = [x['val_loss'] for x in outputs]
        epoch_loss = torch.stack(batch_losses).mean()
        batch_accs = [x['val_acc'] for x in outputs]
        epoch_acc = torch.stack(batch_accs).mean()
        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}

    def epoch_end(self, epoch, result):
        print("Epoch [{}], train_loss: {:.4f}, train_acc: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}, last_lr: {:.5f}".format(
            epoch+1, result['train_loss'], result['train_accuracy'], result['val_loss'], result['val_acc'], result['lrs'][-1]))

class ResNet18Embedder(nn.Module):
    def __init__(self):
        super().__init__()

        resnet = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)
        self.features = nn.Sequential(*list(resnet.children())[:-1])

    def forward(self, x):

        x = self.features(x)
        x = x.view(x.size(0), -1)
        return x

class ArcFace(nn.Module):
    def __init__(self, in_features, num_classes, s=30.0, m=0.5):
        super().__init__()
        self.s = s
        self.m = m
        self.weight = nn.Parameter(torch.randn(num_classes, in_features))
        nn.init.xavier_uniform_(self.weight)
        self.num_classes = num_classes

    def forward(self, embeddings, labels):
        embeddings = F.normalize(embeddings)
        W = F.normalize(self.weight)

        cos_theta = torch.matmul(embeddings, W.t()).clamp(-1, 1)

        if self.training:

            theta = torch.acos(cos_theta)
            cos_theta_m = torch.cos(theta + self.m)

            one_hot = F.one_hot(labels, num_classes=self.num_classes).float()
            output_cos = one_hot * cos_theta_m + (1 - one_hot) * cos_theta
        else:

            output_cos = cos_theta

        logits = self.s * output_cos
        return logits

class ArcFaceCombinedModel(ImageClassificationBase):
    def __init__(self, num_classes, embedding_size=512):
        super().__init__()
        self.embedder = ResNet18Embedder()
        self.arcface = ArcFace(embedding_size, num_classes)

    def forward(self, images, labels=None):
        embeddings = self.embedder(images)

        if self.training and labels is not None:
            output = self.arcface(embeddings, labels)
        else:
            self.arcface.eval()
            output = self.arcface(embeddings, labels=None)

            if self.training:
                self.arcface.train()
        return output

print("ArcFaceCombinedModel class defined successfully.")



def accuracy(outputs, labels):
    _, preds = torch.max(outputs, dim=1)
    return torch.tensor(torch.sum(preds == labels).item() / len(preds))

def get_lr(optimizer):
    for param_group in optimizer.param_groups:
        return param_group['lr']

@torch.no_grad()
def evaluate(model, val_loader):
    model.eval()
    outputs = []
    for batch in val_loader:

        outputs.append(model.validation_step(batch))
    return model.validation_epoch_end(outputs)

def fit_one_cycle(epochs, max_lr, model, train_loader, val_loader, optimizer, grad_clip=None, weight_decay=0, start_pct=0.3, moms=(0.95, 0.85), pct_start=0.3, div_factor=25, final_div_factor=1e4):
    torch.cuda.empty_cache()
    history = []

    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr,
                                                    epochs=epochs, steps_per_epoch=len(train_loader),
                                                    pct_start=pct_start,
                                                    div_factor=div_factor,
                                                    final_div_factor=final_div_factor,
                                                    anneal_strategy='cos')

    best_val_acc = -np.inf
    best_model_path = 'best_model_arcface.pth'

    for epoch in range(epochs):
        model.train()
        train_losses = []
        train_accs = []
        lrs = []

        for batch in tqdm(train_loader):
            optimizer.zero_grad()

            loss, acc = model.training_step(batch)
            loss.backward()

            if grad_clip:
                nn.utils.clip_grad_value_(model.parameters(), grad_clip)

            optimizer.step()


            train_losses.append(loss.item())
            train_accs.append(acc.item())

            lrs.append(get_lr(optimizer))
            scheduler.step()

        #  Валидация
        result = evaluate(model, val_loader)
        result['train_loss'] = np.mean(train_losses)
        result['train_accuracy'] = np.mean(train_accs)
        result['lrs'] = lrs
        model.epoch_end(epoch, result)
        history.append(result)

        if result['val_acc'] > best_val_acc:
            best_val_acc = result['val_acc']
            torch.save(model.state_dict(), best_model_path)
            print(f"Saving best ArcFace model with validation accuracy: {best_val_acc:.4f}")

    return history

if not torch.cuda.is_available():
    raise RuntimeError("CUDA is not available. Please enable GPU runtime in Colab settings to train on GPU.")

epochs = 40
grad_clip = 0.1
weight_decay = 1e-4
max_lr = 0.001

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

arcface_model = ArcFaceCombinedModel(num_classes=num_classes)
arcface_model.to(device);

print(f"ArcFaceCombinedModel instantiated for {num_classes} classes and moved to {device}.")

optimizer_arcface = torch.optim.AdamW(arcface_model.parameters(), lr=max_lr, weight_decay=weight_decay)
print(f"New optimizer (AdamW) initialized for the ArcFace model with learning rate: {max_lr}")

history_arcface = fit_one_cycle(epochs, max_lr, arcface_model, train_dl, val_dl, optimizer_arcface, grad_clip=grad_clip, weight_decay=weight_decay)

print("ArcFace model training complete!")

final_model_path = 'final_arcface_model.pth'
torch.save(arcface_model.state_dict(), final_model_path)
print(f"Final ArcFace model saved to {final_model_path}")

"""**Оценка обучения модели на ArcFace loss:**

Для обучения (на CE loss и на ArcFace loss) была взята предобученная модель ResNet18.
После обучения модели с ArcFace loss, validation accuracy составила 0.7443, что превышает validation accuracy (0.7383), полученную при обучении модели на CE loss. Однако, для достижения порога accuracy в 0,7 обучени модели на CE loss хватило 20 эпох, для модели на ArcFace loss - 40 эпох.
"""