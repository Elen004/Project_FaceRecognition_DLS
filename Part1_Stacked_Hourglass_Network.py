# -*- coding: utf-8 -*-
"""DLS_Проект_(2)_ Stacked Hourglass Network.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sVeXBb57hCFejvC7L2cWPaZy2y74Aw1N

#**Реализация и обучение Stacked Hourglass Network - модели для поиска ключевых точек лица**

Импортируем нужные библиотеки:
"""

import pandas as pd
import numpy as np
import matplotlib as plt
import torch
import torch.nn as nn
from torchvision.datasets import ImageFolder
import torchvision.transforms as tt
import matplotlib.pyplot as plt
import os
import random
from PIL import Image

"""Доступ к диску для загрузки файлов:"""

from google.colab import drive
drive.mount('/content/drive')

"""# Принцип формирования датасета

**Критерии выбора изображений для формирования исходного датасета (для задачи Face Recognition):**

Для обучения модели, соглано заданию, был взят датасет CelebA. Однако, размер даатсета слишком велик (около 200 000 изображений), поэтому было решено сделать выборку изображений для обучения модели размером 10 000 - 20 000 изабражений.

Для полного датасета CelebA - Original Wild Images имеется файл list_attr_celeba.csv (https://www.kaggle.com/datasets/kevinpatel04/celeba-original-wild-images), в котором обозначены определённые атриботы исходных изображенй.   

Однако, не все атрибуты представляют ценность при решении задачи распознования лиц. Так, атрибуты которые касаются внешнего вида одежды, причёски, бижутерии нет смысла учитывать, так как они будут "обрезаны" на следующих этапах.

Также атрибуты слишком обобщенной формулировкой не будут для нас информативны. Например, атрибут "Male" не представляет особой важности. Лица с атрибутом   "Male" будут учтены и при отборе изображений с атрибутами "5_o_Clock_Shadow" и, скорее всего, при отборе лиц с другими атрибутами, такими как "Big_Nose", "Narrow_Eyes" и др.

 Полный список атрибутов:

 "+" - учитываем при формировании датасета

 "-" - не учитываем при формировании датасета

*   5_o_Clock_Shadow (+)
*   Arched_Eyebrows (+)
*   Attractive (-, слишком "общая категория")
*   Bags_Under_Eyes (+)
*   Bald (-, причёска)
*   Bangs (-, причёска)
*   Big_Lips (+)
*   Big_Nose(+)
*   Black_Hair(-, причёска)
*   Blond_Hair (-, причёска)
*   Blurry(+)
*   Brown_Hair (-, причёска)
*   Bushy_Eyebrows(+)
*   Chubby  (-, слишком "общая категория")
*   Double_Chin (+)
*   Eyeglasses (+)
*   Goatee (+)
*   Gray_Hair (-, причёска)
*   Heavy_Makeup (+)
*   High_Cheekbones (+)
*   Male  (-, слишком "общая категория")
*   Mouth_Slightly_Open (+)
*   Mustache (+)
*   Narrow_Eyes (+)
*   No_Beard (+)
*   Oval_Face (+)
*   Pale_Skin (+)
*   Pointy_Nose (+)
*   Receding_Hairline (-, причёска)
*   Rosy_Cheeks (+)
*   Sideburns (+)
*   Smiling (+)
*   Straight_Hair (-, причёска)
*   Wavy_Hair (-, причёска)
*   Wearing_Earrings (-, предмет гардероба)
*   Wearing_Hat (-, предмет гардероба)
*   Wearing_Lipstick (+)
*   Wearing_Necklace (-, предмет гардероба)
*   Wearing_Necktie (-, предмет гардероба)
*   Young (-, слишком "общая категория")

Таким образом, информативными для нас являются 23 атрибута из 40. Мы хотим составить обучающую выборку из различных изображений, т.е. хотим включить в неё изобрадения со веми "информативными" атрибутами, чтобы модель была обучена  хорошо различать лица с некоторыми отличительными признаками.

Для разбиения изображений по классам использовали файл identity_CelebA.txt(взят с официального сайта  авторов полного датасета Celeb_A https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html).

Для обучения сформировали датасет из 1500 классов. Отобрали изображения таким образом, чтобы в каждом классе было от 10 до 15 изображений. Всего было отобрано 20 000 изображений.

При отборе учитывали информативные атрибуты. Было посчитатно, сколько изображений приходится на каждый информативный атрибут (одно изображение может характеризоваться несколькими атрибутами):

- 5_o_Clock_Shadow: 2270  
- Arched_Eyebrows: 5107  
- Bags_Under_Eyes: 3968  
- Big_Lips: 4141  
- Big_Nose: 4776  
- Blurry: 923  
- Bushy_Eyebrows: 3096  
- Double_Chin: 825
- Eyeglasses: 1246  
- Goatee: 1325  
- Heavy_Makeup: 8011  
- High_Cheekbones: 9362  
- Mouth_Slightly_Open: 9529  
- Mustache: 788  
- Narrow_Eyes: 2212  
- No_Beard: 16684  
- Oval_Face: 6332  
- Pale_Skin: 792  
- Pointy_Nose: 5643  
- Rosy_Cheeks: 1357  
- Sideburns: 1184  
- Smiling: 10070  
- Wearing_Lipstick: 9464

# Загрузка данных: датасет изображений, bbox, landmarks

Загрузим датасет для дальнейшей работы:
"""

path_ds_for_fr = '/content/drive/MyDrive/ Курсы DLS/Итоговый проект dls/ Датасет_вариант 2/selected_image_(2).zip'

image_extensions = ('.png', '.jpg', '.jpeg', '.gif', '.bmp', '.tiff')
image_files = [f for f in os.listdir(path_ds_for_fr) if f.lower().endswith(image_extensions)]
num_images = len(image_files)
print(f"Количество изображений в '{path_ds_for_fr}': {num_images}")

import zipfile

def file_from_zip(zip_path):

  extract_path = '/content/selected_images' # Директория для разархивации

  # сохраняем разорхивированный файл во временную среду Colab
  os.makedirs(extract_path, exist_ok=True)

  with zipfile.ZipFile(zip_path, 'r') as zip_ref:
      zip_ref.extractall(extract_path)
  print(f"Файл успешно разархивирован в: {extract_path}")

  ds_for_fr_unzip = extract_path

  return ds_for_fr_unzip

path_ds_for_fr = file_from_zip(path_ds_for_fr)

image_size = 128    # Размер изображений
stats = ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])      # Среднее значение и стандартное отклонение для нормализации (стандартные для предварительно обученных моделей ImageNet)

#  Список изображений
image_files = [os.path.join(path_ds_for_fr, f) for f in os.listdir(path_ds_for_fr) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]

# Количество выводимых изображений
num_images_to_show = 4
selected_images = image_files[:num_images_to_show]

# Преобразование изображений в tensor
load_and_transform = tt.Compose([
    tt.Resize(image_size),
    tt.CenterCrop(image_size),
    tt.ToTensor(),
])

# Денормализация
def denormalize(image_tensor, means, stds):
    means = torch.tensor(means).reshape(3, 1, 1)
    stds = torch.tensor(stds).reshape(3, 1, 1)
    return image_tensor * stds + means

plt.figure(figsize=(12, 3))

for i, img_path in enumerate(selected_images):
    #  Подгружаем изображение
    img = Image.open(img_path).convert('RGB')
    img_tensor = load_and_transform(img)

    #  Нормализуем
    normalized_img_tensor = tt.Normalize(*stats)(img_tensor)

    #  Денормализуем для отображения
    denormalized_img = denormalize(normalized_img_tensor, stats[0], stats[1])

    img_np = np.clip(denormalized_img.permute(1, 2, 0).numpy(), 0, 1)

    plt.subplot(1, num_images_to_show, i + 1)
    plt.imshow(img_np)
    plt.title(os.path.basename(img_path))
    plt.axis('off')

plt.tight_layout()
plt.show()

"""Для всех изображений из датасета CelebA уже имеются координаты их bbox (файл list_landmarks_celeba с сайта https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html)."""

bbox = pd.read_csv(
    '/content/drive/MyDrive/ Курсы DLS/Итоговый проект dls/list_bbox_celeba.txt',
    sep=r'\s+',
    skiprows=1,
    header=0,
    dtype={'x_1': int, 'y_1': int, 'width': int, 'height': int}
)

bbox

"""Файл, содержащий значения landmarks: list_landmarks_celeba.txt (взят с сайта авторров датасета CelebA https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html)"""

landmarks = pd.read_csv(
    '/content/drive/MyDrive/ Курсы DLS/Итоговый проект dls/list_landmarks_celeba.txt',
    sep=r'\s+',
    skiprows=1,
    header=0,
    dtype={
        'lefteye_x': int, 'lefteye_y': int,
        'righteye_x': int, 'righteye_y': int,
        'nose_x': int, 'nose_y': int,
        'leftmouth_x': int, 'leftmouth_y': int,
        'rightmouth_x': int, 'rightmouth_y': int
    }
)
landmarks

"""Также загрузим csv файл с оригинальными наименованиями изображений (image_id),отобраных в обучабщий датасет:"""

selected_img_name = pd.read_csv('/content/drive/MyDrive/ Курсы DLS/Итоговый проект dls/ Датасет_вариант 2/selected_images_with_classes_2.csv')

print("Current columns in selected_img_name:", selected_img_name.columns.tolist())

if 'class_id' in selected_img_name.columns:
    selected_img_name.drop('class_id', axis=1, inplace=True)
    print("'class_id' column dropped successfully.")
else:
    print("'class_id' column not found in DataFrame. It might have already been dropped or the name is different.")

print("Columns after operation:", selected_img_name.columns.tolist())

selected_img_name

"""Выберем нужные bbox-ы и landmark-и по image_id изображений:"""

bbox.set_index('image_id', inplace=True)
print("Set 'image_id' as index for bbox DataFrame.")

selected_img_name.set_index('image_id', inplace=True)
filtered_bbox = bbox.merge(selected_img_name, left_index=True, right_index=True, how='inner')
print("Merged selected image names with bbox to create filtered_bbox.")

combined_df = filtered_bbox.merge(landmarks, left_index=True, right_index=True, how='inner')
print("Merged filtered_bbox with landmarks to create combined_df.")

print("First 5 rows of combined_df:")
print(combined_df.head())
print("\nShape of combined_df:")
print(combined_df.shape)

"""Напишем функцию для обрезки изображений по bbox и пересчёта координат landmarcks:"""

def crop_and_adjust_landmarks(image_path, bbox_coords, landmark_coords):
    """
    Обрезка изображений по bbox, пересчёт коордитат landmarcks

    Аргументы:
        image_path (str): Путь к файлу изображения.
        bbox_coords (dict): cловарь с ключами 'x_1', 'y_1', 'width', 'height'.
        landmark_coords (dict): cловарь с исходными координатами bbox (например, 'lefteye_x', 'lefteye_y').

    Retutn:
        кортеж:
            - PIL.Обрезанные изображения
            - словарь с измененными координатами ориентира.
    """
    # 2. Загрузим изображения
    img = Image.open(image_path).convert('RGB')

    # 3. Посчитаем xmax и ymax для bounding box
    x1, y1, width, height = bbox_coords['x_1'], bbox_coords['y_1'], bbox_coords['width'], bbox_coords['height']
    x2 = x1 + width
    y2 = y1 + height

    # 4.  Обрежем изображения
    cropped_img = img.crop((x1, y1, x2, y2))

    adjusted_landmark_coords = {}

    # 5. Пересчитаем координаты landmark
    for key, value in landmark_coords.items():
        if '_x' in key:
            adjusted_landmark_coords[key] = value - x1
        elif '_y' in key:
            adjusted_landmark_coords[key] = value - y1
        else:
            adjusted_landmark_coords[key] = value

    return cropped_img, adjusted_landmark_coords

print("Defined function 'crop_and_adjust_landmarks'.")

cropped_images_dir = '/content/cropped_images'
os.makedirs(cropped_images_dir, exist_ok=True)
print(f"Created directory: {cropped_images_dir}")

adjusted_landmarks_list = []
print("Initialized 'adjusted_landmarks_list'.")

for image_id, row in combined_df.iterrows():

    # Получим нужные координаты bbox:
    bbox_coords = {
        'x_1': row['x_1'],
        'y_1': row['y_1'],
        'width': row['width'],
        'height': row['height']
    }

    # Получиим координаты landmarks

    landmark_cols = [col for col in combined_df.columns if col.startswith(('lefteye', 'righteye', 'nose', 'leftmouth', 'rightmouth'))]
    landmark_coords = {col: row[col] for col in landmark_cols}

    original_image_path = os.path.join(path_ds_for_fr, image_id)

    #  Вызов функции crop_and_adjust_landmarks
    cropped_img, adjusted_landmark_coords = crop_and_adjust_landmarks(
        original_image_path, bbox_coords, landmark_coords
    )

    #  Сохранение обрезанного изображения
    save_path = os.path.join(cropped_images_dir, image_id)
    cropped_img.save(save_path)

    adjusted_landmark_coords['image_id'] = image_id
    adjusted_landmarks_list.append(adjusted_landmark_coords)

#  Таблица adjusted_landmarks_list
adjusted_landmarks_df = pd.DataFrame(adjusted_landmarks_list)
adjusted_landmarks_df.set_index('image_id', inplace=True)

print("Processed all selected images: cropped, adjusted landmarks, and saved.")
print("Adjusted landmarks DataFrame created and indexed by 'image_id'.")

""" Посмотрим на результат:

"""

num_samples =  5

if len(adjusted_landmarks_df) > num_samples:
    sample_image_ids = random.sample(list(adjusted_landmarks_df.index), num_samples)
else:
    sample_image_ids = list(adjusted_landmarks_df.index)

plt.figure(figsize=(num_samples * 3, 4))

for i, image_id in enumerate(sample_image_ids):
    # Загрузим обрезанное изображение
    image_path = os.path.join(cropped_images_dir, image_id)
    img = Image.open(image_path).convert('RGB')

    #  Пересчитаем координаты точек landmark
    landmarks_row = adjusted_landmarks_df.loc[image_id]
    x_coords = [landmarks_row[col] for col in landmarks_row.index if '_x' in col]
    y_coords = [landmarks_row[col] for col in landmarks_row.index if '_y' in col]

    #  Отобразиим результат
    plt.subplot(1, num_samples, i + 1)
    plt.imshow(img)
    plt.scatter(x_coords, y_coords, c='red', s=10, marker='o', label='Landmarks') # s is marker size
    plt.title(f'Cropped: {image_id}')
    plt.axis('off')

plt.tight_layout()
plt.show()

"""Результат: ключевые точеки на образенных изображениях отображены корректно, соответственно, верно выполнен пересчёт кооррдинат landmarks

Функция для создания тепловых карт (heatmap):
"""

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(device)

def create_heatmap(size, landmark, sigma=2):
    """
    Создаёт один heatmap с гауссовым ядром вокруг точки.

    :param size: (height, width) — размер heatmap'а
    :param landmark:(x, y) — координаты точки
    :param sigma
    :return: heatmap массив
    """
    x, y = landmark
    h, w = size

    # Обрезаем координаты, чтобы не выйти за пределы изображения
    x = min(max(0, int(x)), w - 1)
    y = min(max(0, int(y)), h - 1)

    xx, yy = np.meshgrid(np.arange(w), np.arange(h))
    heatmap = np.exp(-((yy - y)**2 + (xx - x)**2) / (2 * sigma**2))
    return heatmap


def landmarks_to_heatmaps(image_shape, landmarks, sigma=2):
    """
    Преобразует список из N точек в набор из N heatmap'ов.

    :param image_shape: исходный размер изображения (H, W)
    :param landmarks: список из N пар координат [(x1, y1), (x2, y2), ..., (xN, yN),]
    :param sigma:
    :return: массив heatmap'ов вида [N, H, W]
    """
    heatmaps = []

    for (x, y) in landmarks:
        hm = create_heatmap(image_shape, sigma=sigma)
        heatmaps.append(hm)

    return np.array(heatmaps)

"""---

# Реализация Stacked Hourglass Network
"""

class ResidualBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.skip = nn.Identity() if in_channels == out_channels else nn.Conv2d(in_channels, out_channels, 1)

        self.conv1 = nn.Conv2d(in_channels, out_channels // 2, 1)
        self.bn1 = nn.BatchNorm2d(out_channels // 2)
        self.conv2 = nn.Conv2d(out_channels // 2, out_channels // 2, 3, padding=1)
        self.bn2 = nn.BatchNorm2d(out_channels // 2)
        self.conv3 = nn.Conv2d(out_channels // 2, out_channels, 1)
        self.bn3 = nn.BatchNorm2d(out_channels)

        self.relu = nn.ReLU(inplace=True)

    def forward(self, x):
        residual = self.skip(x)
        x = self.relu(self.bn1(self.conv1(x)))
        x = self.relu(self.bn2(self.conv2(x)))
        x = self.bn3(self.conv3(x))
        return self.relu(x + residual)

class Hourglass (nn.Module):
    def __init__(self, n, f, bn=None, increase=0):
        super().__init__()

        nf = f + increase
        self.up1 = Residual(f, f)
        self.pool1 = Pool(2, 2)
        self.low1 = Residual(f, nf)
        self.n = n
        if self.n > 1:
            self.low2 = Hourglass(n-1, nf, bn=bn)
        else:
            self.low2 = Residual(nf, nf)
        self.low3 = Residual(nf, f)
        self.up2 = nn.Upsample(scale_factor=2, mode='nearest')

    def forward(self, x):
        up1  = self.up1(x)
        pool1 = self.pool1(x)
        low1 = self.low1(pool1)
        low2 = self.low2(low1)
        low3 = self.low3(low2)
        up2  = self.up2(low3)
        return up1 + up2

"""Реализуем архитектуру Stacked Hourglass Network, используя class ResidualBlock. На выходе каждого Hourglass-блока находится слой (голова), который создает heatmap:"""

class ResidualBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.skip = nn.Identity() if in_channels == out_channels else nn.Conv2d(in_channels, out_channels, 1)

        self.conv1 = nn.Conv2d(in_channels, out_channels // 2, 1)
        self.bn1 = nn.BatchNorm2d(out_channels // 2)
        self.conv2 = nn.Conv2d(out_channels // 2, out_channels // 2, 3, padding=1)
        self.bn2 = nn.BatchNorm2d(out_channels // 2)
        self.conv3 = nn.Conv2d(out_channels // 2, out_channels, 1)
        self.bn3 = nn.BatchNorm2d(out_channels)

        self.relu = nn.ReLU(inplace=True)

    def forward(self, x):
        residual = self.skip(x)
        x = self.relu(self.bn1(self.conv1(x)))
        x = self.relu(self.bn2(self.conv2(x)))
        x = self.bn3(self.conv3(x))
        return self.relu(x + residual)


class HourglassModule(nn.Module):
    def __init__(self, n, f, bn=None, increase=0):
        super().__init__()
        self.n = n
        nf = f + increase

        self.up1 = ResidualBlock(f, f)
        self.pool1 = nn.MaxPool2d(2, 2)
        self.low1 = ResidualBlock(f, nf)

        if self.n > 1:
            self.low2 = HourglassModule(self.n - 1, nf, bn=bn)
        else:
            self.low2 = ResidualBlock(nf, nf)

        self.low3 = ResidualBlock(nf, f)
        self.up2 = nn.Upsample(scale_factor=2, mode='nearest')

    def forward(self, x):
        up1 = self.up1(x)
        pool1 = self.pool1(x)
        low1 = self.low1(pool1)
        low2 = self.low2(low1)
        low3 = self.low3(low2)
        up2 = self.up2(low3)
        return up1 + up2


class StackedHourglassNet(nn.Module):
    def __init__(self, num_stacks, num_channels, num_landmarks):
        super().__init__()
        self.num_stacks = num_stacks
        self.num_channels = num_channels
        self.num_landmarks = num_landmarks

        self.initial_features = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            ResidualBlock(64, 128),
            nn.MaxPool2d(2, 2),
            ResidualBlock(128, num_channels),
            ResidualBlock(num_channels, num_channels)
        )

        # Stacked Hourglass Blocks
        self._hourglass = nn.ModuleList()
        self._intermediate_res = nn.ModuleList()
        self._heatmap_heads = nn.ModuleList()
        self._feature_convs = nn.ModuleList()
        self._heatmap_convs = nn.ModuleList()

        for i in range(num_stacks):
            self._hourglass.append(HourglassModule(n=4, f=num_channels))
            self._intermediate_res.append(ResidualBlock(num_channels, num_channels))
            self._heatmap_heads.append(nn.Conv2d(num_channels, num_landmarks, kernel_size=1))

            if i < num_stacks - 1:
                self._feature_convs.append(nn.Conv2d(num_channels, num_channels, kernel_size=1))
                self._heatmap_convs.append(nn.Conv2d(num_landmarks, num_channels, kernel_size=1))

    def forward(self, x):
        x = self.initial_features(x)

        outputs = []
        for i in range(self.num_stacks):
            hg_output = self._hourglass[i](x)
            hg_output = self._intermediate_res[i](hg_output)

            heatmap_output = self._heatmap_heads[i](hg_output)
            outputs.append(heatmap_output)

            if i < self.num_stacks - 1:
                feature_proj = self._feature_convs[i](hg_output)
                heatmap_proj = self._heatmap_convs[i](heatmap_output)
                x = x + feature_proj + heatmap_proj

        return outputs

print("Defined HourglassModule and StackedHourglassNet classes.")

"""---

# Face alignment - выравнивание по точкам

Алгоритм face alignment, взятый за основу
https://www.pythontutorials.net/blog/how-would-i-achieve-this-in-opencv-with-an-affine-transform/

Объединим предыдущие шаги и отобразим полученные heatmap:

В ходе работы возникали ошибки с обработкрй некоторых изображений. Прилось отдельно учитывать изображения, для которых удалось получить heatmap-ы (Successfully generated and saved heatmaps) и работать только с ними.

P.S.: Ошибку с обработкой изображений удалось устранить, в итоге удалось получить heatmap-ы для всех 20 000 изображений из датасета
"""

import numpy as np
from PIL import Image
import os
import matplotlib.pyplot as plt
import random
import pandas as pd
import zipfile
import torchvision.transforms as tt
import torch
from torch.utils.data import DataLoader, Dataset


image_size = 64
stats = ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])

# StackedHourglassNet имеет начальную свертку (шаг 2) и максимальное объединение (шаг 2),общbq коэффициент понижающей дискретизации, равен 4. Таким образом, 64/4 = 16.
output_heatmap_size = image_size // 4 # e.g., 64 // 4 = 16

#  Разархивируем zip файл
def file_from_zip(zip_path):
  extract_path = '/content/selected_images'
  os.makedirs(extract_path, exist_ok=True)
  with zipfile.ZipFile(zip_path, 'r') as zip_ref:
      zip_ref.extractall(extract_path)
  return extract_path

#  Выполняем на графическом процессоре cuda
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

# Доступ к файлам
dls_project_folder_path = '/content/drive/MyDrive/ Курсы DLS/Итоговый проект dls/'
zip_source_path = os.path.join(dls_project_folder_path, 'selected_image_(2).zip')

path_ds_for_fr = '/content/selected_images'

if not os.path.exists(path_ds_for_fr) or not os.listdir(path_ds_for_fr):
    print(f"Unzipping {zip_source_path} to {path_ds_for_fr}...")
    if os.path.exists(zip_source_path):
        file_from_zip(zip_source_path)
    else:
        print(f"Warning: {zip_source_path} not found. Please ensure it's in your Google Drive.")

try:
    bbox = pd.read_csv(
        os.path.join(dls_project_folder_path, 'list_bbox_celeba.txt'),
        sep=r'\s+', skiprows=1, header=0, dtype={'x_1': int, 'y_1': int, 'width': int, 'height': int}
    )
    landmarks = pd.read_csv(
        os.path.join(dls_project_folder_path, 'list_landmarks_celeba.txt'),
        sep=r'\s+', skiprows=1, header=0, dtype={
            'lefteye_x': int, 'lefteye_y': int, 'righteye_x': int, 'righteye_y': int,
            'nose_x': int, 'nose_y': int, 'leftmouth_x': int, 'leftmouth_y': int,
            'rightmouth_x': int, 'rightmouth_y': int
        }
    )

    selected_img_name = pd.read_csv(os.path.join(dls_project_folder_path, ' Датасет_вариант 2/selected_images_with_classes_2.csv'))

    #  Работа с изображениями и bbox
    if 'class_id' in selected_img_name.columns:
        selected_img_name.drop('class_id', axis=1, inplace=True)

    bbox.set_index('image_id', inplace=True)
    selected_img_name.set_index('image_id', inplace=True)
    filtered_bbox = bbox.merge(selected_img_name, left_index=True, right_index=True, how='inner')
    combined_df = filtered_bbox.merge(landmarks, left_index=True, right_index=True, how='inner')
    print("Successfully loaded and merged bounding box and landmark dataframes.")
except FileNotFoundError as e:
    print(f"CRITICAL ERROR: Data file not found. Please ensure all necessary files are in '{dls_project_folder_path}'. Error: {e}")

    raise

#  Функция обрезки изображений и пересчёта landmarks
def crop_and_adjust_landmarks(image_path, bbox_coords, landmark_coords):
    img = Image.open(image_path).convert('RGB')
    x1, y1, width, height = bbox_coords['x_1'], bbox_coords['y_1'], bbox_coords['width'], bbox_coords['height']
    x2 = x1 + width
    y2 = y1 + height
    cropped_img = img.crop((x1, y1, x2, y2))
    adjusted_landmark_coords = {}
    for key, value in landmark_coords.items():
        if '_x' in key:
            adjusted_landmark_coords[key] = value - x1
        elif '_y' in key:
            adjusted_landmark_coords[key] = value - y1
        else:
            adjusted_landmark_coords[key] = value
    return cropped_img, adjusted_landmark_coords

#  Сохранение результата
cropped_images_dir = '/content/cropped_images'
os.makedirs(cropped_images_dir, exist_ok=True)

adjusted_landmarks_list = []
print("Cropping images and adjusting landmarks...")
for image_id, row in combined_df.iterrows():
    bbox_coords = {'x_1': row['x_1'], 'y_1': row['y_1'], 'width': row['width'], 'height': row['height']}
    landmark_cols = [col for col in combined_df.columns if col.startswith(('lefteye', 'righteye', 'nose', 'leftmouth', 'rightmouth'))]
    landmark_coords = {col: row[col] for col in landmark_cols}
    original_image_path = os.path.join(path_ds_for_fr, image_id)

    if not os.path.exists(original_image_path):
        continue

    try:
        cropped_img, adjusted_landmark_coords = crop_and_adjust_landmarks(
            original_image_path, bbox_coords, landmark_coords
        )
        save_path = os.path.join(cropped_images_dir, image_id)
        cropped_img.save(save_path)
        adjusted_landmark_coords['image_id'] = image_id
        adjusted_landmarks_list.append(adjusted_landmark_coords)
    except Exception as e:
        pass

adjusted_landmarks_df = pd.DataFrame(adjusted_landmarks_list)
adjusted_landmarks_df.set_index('image_id', inplace=True)
print(f"Finished cropping and adjusting landmarks for {len(adjusted_landmarks_df)} images.")

#  Создание тепловых карт
def create_heatmap(size, landmark, sigma=2):
    x, y = landmark
    h, w = size
    x = min(max(0, int(x)), w - 1)
    y = min(max(0, int(y)), h - 1)
    xx, yy = np.meshgrid(np.arange(w), np.arange(h))
    heatmap = np.exp(-((yy - y)**2 + (xx - x)**2) / (2 * sigma**2))
    return heatmap

#  Перевод landmarks в heatmaps
def landmarks_to_heatmaps(original_img_shape, landmarks, target_heatmap_size, sigma=2):
    heatmaps = []
    orig_h, orig_w = original_img_shape

    for (x_orig, y_orig) in landmarks:

        x_scaled = x_orig * (target_heatmap_size / orig_w)
        y_scaled = y_orig * (target_heatmap_size / orig_h)

        x_scaled = max(0, min(x_scaled, target_heatmap_size - 1))
        y_scaled = max(0, min(y_scaled, target_heatmap_size - 1))

        hm = create_heatmap((target_heatmap_size, target_heatmap_size), (x_scaled, y_scaled), sigma=sigma)
        heatmaps.append(hm)
    return np.array(heatmaps)

heatmaps_dir = '/content/generated_heatmaps'
os.makedirs(heatmaps_dir, exist_ok=True)
print(f"Created directory for heatmaps: {heatmaps_dir}")

successful_image_ids = []

print("Generating heatmaps for all cropped images and saving to disk...")

if not adjusted_landmarks_df.empty:
    for image_id, landmarks_row in adjusted_landmarks_df.iterrows():
        image_path = os.path.join(cropped_images_dir, image_id)
        if not os.path.exists(image_path):
            continue

        try:
            img = Image.open(image_path)
            img_width, img_height = img.size
            original_img_shape = (img_height, img_width)
        except Exception as e:
            continue

        landmarks_list_for_image = []
        landmark_coords_x = [landmarks_row[col] for col in landmarks_row.index if col.endswith('_x')]
        landmark_coords_y = [landmarks_row[col] for col in landmarks_row.index if col.endswith('_y')]

        if len(landmark_coords_x) != len(landmark_coords_y) or not landmark_coords_x:
            continue

        for x, y in zip(landmark_coords_x, landmark_coords_y):
            landmarks_list_for_image.append((x, y))

        if not landmarks_list_for_image:
            continue

        heatmaps_for_single_image = landmarks_to_heatmaps(
            original_img_shape, landmarks_list_for_image, output_heatmap_size, sigma=2
        )

        # Сохранение heatmap
        heatmap_filename = os.path.join(heatmaps_dir, f"{image_id.replace('.jpg', '')}.npy")
        np.save(heatmap_filename, heatmaps_for_single_image)
        successful_image_ids.append(image_id)
else:
    print("adjusted_landmarks_df is empty, skipping heatmap generation.")

print(f"Successfully generated and saved heatmaps for {len(successful_image_ids)} images.")


#  Преобразование изображений
print(f"DEBUG: image_size used for transforms: {image_size}")
image_transforms = tt.Compose([
    tt.Resize((image_size, image_size)),
    tt.ToTensor(),
    tt.Normalize(*stats)
])

#  Класс FaceLandmarkDataset
class FaceLandmarkDataset(Dataset):
    def __init__(self, image_dir, image_ids, heatmaps_base_dir, transform=None, device='cpu'):
        self.image_dir = image_dir
        self.image_ids = image_ids
        self.heatmaps_base_dir = heatmaps_base_dir
        self.transform = transform
        self.device = device # Store the device

    def __len__(self):
        return len(self.image_ids)

    def __getitem__(self, idx):
        image_id = self.image_ids[idx]

        image_path = os.path.join(self.image_dir, image_id)
        image = Image.open(image_path).convert('RGB')

        if self.transform:
            image = self.transform(image)

        # Загрузим heatmaps
        heatmap_filename = os.path.join(self.heatmaps_base_dir, f"{image_id.replace('.jpg', '')}.npy")
        heatmaps_np = np.load(heatmap_filename)
        heatmaps = torch.from_numpy(heatmaps_np).float().to(self.device)

        image = image.to(self.device)

        return image, heatmaps

print("Defined FaceLandmarkDataset class and image_transforms.")

if successful_image_ids:
    train_dataset = FaceLandmarkDataset(
        image_dir=cropped_images_dir,
        image_ids=successful_image_ids,
        heatmaps_base_dir=heatmaps_dir,
        transform=image_transforms,
        device=device
    )

    print(f"Number of samples in the dataset: {len(train_dataset)}")

    if len(train_dataset) > 0:
        sample_image, sample_heatmaps = train_dataset[0]

        print(f"\nSample image tensor shape: {sample_image.shape}")
        print(f"Sample image tensor type: {sample_image.dtype}, device: {sample_image.device}")
        print(f"Sample heatmaps tensor shape: {sample_heatmaps.shape}")
        print(f"Sample heatmaps tensor type: {sample_heatmaps.dtype}, device: {sample_heatmaps.device}")

        batch_size = 4
        train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

        for images_batch, heatmaps_batch in train_dataloader:
            print(f"\nBatch image tensor shape: {images_batch.shape}, device: {images_batch.device}")
            print(f"Batch heatmaps tensor shape: {heatmaps_batch.shape}, device: {heatmaps_batch.device}")
            break
    else:
        print("Dataset is empty after processing, skipping sample and DataLoader verification.")
else:
    print("No successful heatmaps generated, skipping dataset and dataloader creation.")

print("Successfully created and verified FaceLandmarkDataset and DataLoader (with device handling).")


if successful_image_ids:
    sample_image_id = random.choice(successful_image_ids)

    sample_heatmaps_for_vis = np.load(os.path.join(heatmaps_dir, f"{sample_image_id.replace('.jpg', '')}.npy"))

    print(f"\nVisualizing heatmaps for sample image: {sample_image_id}")
    print(f"Shape of heatmaps for {sample_image_id}: {sample_heatmaps_for_vis.shape}")

    img_original = Image.open(os.path.join(cropped_images_dir, sample_image_id)).convert('RGB')
    landmarks_row = adjusted_landmarks_df.loc[sample_image_id]

    num_landmarks = sample_heatmaps_for_vis.shape[0]

    fig, axes = plt.subplots(1, num_landmarks + 1, figsize=(4 * (num_landmarks + 1), 5))

    axes[0].imshow(img_original)

    x_coords = [landmarks_row[col] for col in landmarks_row.index if col.endswith('_x')]
    y_coords = [landmarks_row[col] for col in landmarks_row.index if col.endswith('_y')]
    axes[0].scatter(x_coords, y_coords, c='red', s=20, marker='o', edgecolors='white', linewidth=0.5)
    axes[0].set_title(f"Cropped: {sample_image_id}\nwith Landmarks")
    axes[0].axis('off')

    for j in range(num_landmarks):
        axes[j+1].imshow(img_original, alpha=0.5)
        axes[j+1].imshow(sample_heatmaps_for_vis[j], cmap='jet', alpha=0.5)

        x_orig = x_coords[j]
        y_orig = y_coords[j]
        axes[j+1].scatter(x_orig, y_orig, c='red', s=20, marker='o', edgecolors='white', linewidth=0.5)

        landmark_name = landmarks_row.index[j*2].replace('_x', '') if j*2 < len(landmarks_row.index) else f"Landmark {j+1}"
        axes[j+1].set_title(f"Heatmap for {landmark_name}")
        axes[j+1].axis('off')

    plt.tight_layout()
    plt.show()

"""Формируем обучающую и валидационную выборку:"""

from sklearn.model_selection import train_test_split

# разделим датасет на обучающую и тестовую (валидационную) выборки
if successful_image_ids:
    train_ids, val_ids = train_test_split(
        successful_image_ids, test_size=0.2, random_state=42
    )
else:
    train_ids = []
    val_ids = []
    print("Warning: successful_image_ids is empty. Cannot create train/val splits.")

print(f"Total images: {len(successful_image_ids)}")
print(f"Training images: {len(train_ids)}")
print(f"Validation images: {len(val_ids)}")


if train_ids:
    train_dataset = FaceLandmarkDataset(
        image_dir=cropped_images_dir,
        image_ids=train_ids,
        heatmaps_base_dir=heatmaps_dir,
        transform=image_transforms,
        device=device
    )
else:
    train_dataset = []

if val_ids:
    val_dataset = FaceLandmarkDataset(
        image_dir=cropped_images_dir,
        image_ids=val_ids,
        heatmaps_base_dir=heatmaps_dir,
        transform=image_transforms,
        device=device
    )
else:
    val_dataset = []

print(f"Train dataset size: {len(train_dataset)}")
print(f"Validation dataset size: {len(val_dataset)}")

# Создадим DataLoader для трейна и для валидации
train_batch_size = 64
val_batch_size = 32

if train_dataset:
    train_dataloader = DataLoader(
        train_dataset, batch_size=train_batch_size, shuffle=True
    )
else:
    train_dataloader = None

if val_dataset:
    val_dataloader = DataLoader(
        val_dataset, batch_size=val_batch_size, shuffle=False
    )
else:
    val_dataloader = None

print(f"Train DataLoader created with batch size: {train_batch_size}")
print(f"Validation DataLoader created with batch size: {val_batch_size}")

# Напишем функцию для обучения модели
def train_model(model, train_dataloader, val_dataloader, optimizer, loss_function, num_epochs, device):
    model.to(device)

    history = {'train_loss': [], 'val_loss': []}

    for epoch in range(num_epochs):

        print(f"\nEpoch {epoch+1}/{num_epochs}")
        model.train()
        running_train_loss = 0.0

        for batch_idx, (images, target_heatmaps) in enumerate(train_dataloader):
            images = images.to(device)
            target_heatmaps = target_heatmaps.to(device)

            optimizer.zero_grad()
            outputs = model(images)
            total_batch_loss = 0

            for output_heatmap in outputs:
                total_batch_loss += loss_function(output_heatmap, target_heatmaps)

            total_batch_loss.backward()
            optimizer.step()

            running_train_loss += total_batch_loss.item()

        avg_train_loss = running_train_loss / len(train_dataloader)
        history['train_loss'].append(avg_train_loss)

        #  Валидация
        model.eval()
        running_val_loss = 0.0
        with torch.no_grad():
            for images, target_heatmaps in val_dataloader:
                images = images.to(device)
                target_heatmaps = target_heatmaps.to(device)

                outputs = model(images)

                total_batch_val_loss = 0
                for output_heatmap in outputs:
                    total_batch_val_loss += loss_function(output_heatmap, target_heatmaps)

                running_val_loss += total_batch_val_loss.item()

        avg_val_loss = running_val_loss / len(val_dataloader)
        history['val_loss'].append(avg_val_loss)

        print(f"  Avg Train Loss: {avg_train_loss:.4f} | Avg Val Loss: {avg_val_loss:.4f}")

    return history

print("Train/validation datasets and DataLoaders are prepared.")
print("The `train_model` function has been defined.")

"""Обучаем модель:"""

import torch.optim as optim

num_stacks = 1
num_channels = 128
num_landmarks = 5

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

model = StackedHourglassNet(num_stacks, num_channels, num_landmarks)
model.to(device)

optimizer = optim.Adam(model.parameters(), lr=0.001)
loss_function = nn.MSELoss()
num_epochs = 5

print("Starting model training...")

history = train_model(
    model,
    train_dataloader,
    val_dataloader,
    optimizer,
    loss_function,
    num_epochs,
    device
)

print("Model training finished.")
print("Training History:", history)

"""Сохраняем выровненные изображения:"""

import matplotlib.pyplot as plt
import cv2

aligned_images_dir = '/content/aligned_faces_full_dataset'
os.makedirs(aligned_images_dir, exist_ok=True)
print(f"Created directory for aligned images: {aligned_images_dir}")

output_size = (256, 256)

print("Starting alignment and saving for all successful images...")
saved_count = 0

for i, image_id in enumerate(successful_image_ids):
    image_path = os.path.join(cropped_images_dir, image_id)

    #  Обрезанные изображения
    try:
        img_pil = Image.open(image_path).convert('RGB')
        image_np_rgb = np.array(img_pil)
        image_np_bgr = cv2.cvtColor(image_np_rgb, cv2.COLOR_RGB2BGR)
    except Exception as e:
        print(f"Could not load or convert image {image_id}: {e}. Skipping.")
        continue

    # landmarks
    if image_id not in adjusted_landmarks_df.index:
        print(f"Landmarks not found for {image_id}. Skipping.")
        continue
    landmarks_row = adjusted_landmarks_df.loc[image_id]

    try:
        landmarks_coords = np.array([
            [landmarks_row['lefteye_x'], landmarks_row['lefteye_y']],
            [landmarks_row['righteye_x'], landmarks_row['righteye_y']],
            [landmarks_row['nose_x'], landmarks_row['nose_y']],
            [landmarks_row['leftmouth_x'], landmarks_row['leftmouth_y']],
            [landmarks_row['rightmouth_x'], landmarks_row['rightmouth_y']]
        ], dtype=np.float32)
    except KeyError as e:
        print(f"Missing landmark data for {image_id}: {e}. Skipping.")
        continue

    try:
        aligned_face_bgr = align_face(
            image_np_bgr,
            landmarks_coords,
            output_size=output_size,
            desired_eye_distance_factor=0.5,
            desired_eye_y_factor=0.3  # desired_eye_distance_factor и desired_eye_y_factor - ключевые факторы для определения масштаба обрезанного (после выравнивания) изображения
        )
        aligned_face_rgb = cv2.cvtColor(aligned_face_bgr, cv2.COLOR_BGR2RGB)

        #  Сохраняем полученные изображения
        aligned_save_path = os.path.join(aligned_images_dir, f"aligned_{image_id}")
        Image.fromarray(aligned_face_rgb).save(aligned_save_path)
        saved_count += 1

        if saved_count % 100 == 0 or saved_count == len(successful_image_ids):
            print(f"Processed and saved {saved_count}/{len(successful_image_ids)} aligned images.")

    except ValueError as e:
        print(f"Error aligning image {image_id}: {e}. Skipping.")
    except Exception as e:
        print(f"An unexpected error occurred for image {image_id} during alignment or saving: {e}. Skipping.")

print(f"Finished. Total {saved_count} aligned images saved to {aligned_images_dir}")

"""---

Создаём zip с выравненными изображениями:
"""

import zipfile

aligned_images_dir = '/content/aligned_faces_full_dataset'
zip_filename = 'aligned_faces_full_dataset.zip'

print(f"Creating zip file: {zip_filename} from directory: {aligned_images_dir}")

with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:
    for root, dirs, files in os.walk(aligned_images_dir):
        for file in files:
            file_path = os.path.join(root, file);
            zipf.write(file_path, os.path.relpath(file_path, aligned_images_dir));

print(f"Successfully created {zip_filename} containing {len(os.listdir(aligned_images_dir))} aligned images.")

"""Посмотрим на результат:"""

aligned_images_dir = '/content/aligned_faces_full_dataset'

image_files = [f for f in os.listdir(aligned_images_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]

num_images_to_display = 5

if len(image_files) == 0:
    print(f"No images found in {aligned_images_dir}.")
elif len(image_files) < num_images_to_display:
    print(f"Only {len(image_files)} images found. Displaying all of them.")
    selected_files = image_files
else:
    selected_files = random.sample(image_files, num_images_to_display)

plt.figure(figsize=(num_images_to_display * 3, 4))

for i, file_name in enumerate(selected_files):
    img_path = os.path.join(aligned_images_dir, file_name)
    img = Image.open(img_path).convert('RGB')

    plt.subplot(1, num_images_to_display, i + 1)
    plt.imshow(img)
    plt.title(os.path.basename(file_name))
    plt.axis('off')

plt.tight_layout()
plt.show()

"""Сохраним полученную модель:"""

model_save_path = 'stacked_hourglass_net.pth'
model.to('cpu')
torch.save(model.state_dict(), model_save_path)

print(f"Модель успешно сохранена в: {model_save_path}")

""" Представим финальный результат:"""

num_samples_to_visualize = 5
if len(successful_image_ids) < num_samples_to_visualize:
    print(f"Warning: Not enough successful images ({len(successful_image_ids)}) to select {num_samples_to_visualize}. Selecting all available images.")
    sample_image_ids = successful_image_ids
else:
    sample_image_ids = random.sample(successful_image_ids, num_samples_to_visualize)

print(f"Selected {len(sample_image_ids)} sample image IDs: {sample_image_ids}")

cropped_images_dir = '/content/cropped_images'
aligned_images_dir = '/content/aligned_faces_full_dataset'

if 'sample_image_ids' not in locals() or not sample_image_ids:
    print("No sample_image_ids found. Please run the previous step to select sample images.")
else:
    num_samples_to_visualize = len(sample_image_ids)
    fig, axes = plt.subplots(2, num_samples_to_visualize, figsize=(num_samples_to_visualize * 4, 8))
    fig.suptitle('Original Cropped vs. Aligned Faces', fontsize=16)

    for i, image_id in enumerate(sample_image_ids):
        #  Оригинальные изображения
        original_cropped_path = os.path.join(cropped_images_dir, image_id)
        #  Выровненные изображения
        aligned_image_path = os.path.join(aligned_images_dir, f"aligned_{image_id}")

        #  Оригинальные изображения
        try:
            img_original_cropped = Image.open(original_cropped_path).convert('RGB')
        except FileNotFoundError:
            print(f"Warning: Original cropped image {original_cropped_path} not found. Skipping.")
            continue

        #  Выравненные изображения
        try:
            img_aligned = Image.open(aligned_image_path).convert('RGB')
        except FileNotFoundError:
            print(f"Warning: Aligned image {aligned_image_path} not found. Skipping.")
            continue

        #  Оригинальные изображения
        axes[0, i].imshow(img_original_cropped)
        axes[0, i].set_title(f"Original Cropped\n({image_id})")
        axes[0, i].axis('off')

        #  Выравненные изображения
        axes[1, i].imshow(img_aligned)
        axes[1, i].set_title(f"Aligned Face")
        axes[1, i].axis('off')

    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
    plt.show()